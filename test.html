<p data-vmark="2faa">IT之家 4 月 17 日消息，人工智能公司 OpenAI 宣布部署了一套新的监控系统，专门用于监测其最新的人工智能推理模型 o3 和 o4-mini，防止这些模型提供可能引发生物和化学威胁的有害建议。根据 OpenAI 的安全报告，该系统旨在确保模型不会为潜在的有害攻击提供指导。</p><p style="text-align: center;" data-vmark="1da2"><img src="https://img.ithome.com/newsuploadfiles/2025/4/2d0db070-930a-4c66-b161-98911798d135.jpg?x-bce-process=image/format,f_auto" w="1176" h="801" data-weibo="0" data-vmark="5783" class="no-alt-img"></p><p data-vmark="5f42">据 OpenAI 表示，o3 和 o4-mini 相比之前的模型在能力上有显著提升，但同时也带来了新的风险。根据 OpenAI 内部的基准测试，o3 特别擅长回答有关制造某些类型生物威胁的问题。因此，为了降低相关风险，OpenAI 开发了这套被称为“安全导向推理监控器”的新系统。</p><p data-vmark="0f5c">据IT之家了解，<strong>该监控器经过专门训练，能够理解 OpenAI 的内容政策，并运行在 o3 和 o4-mini 之上</strong>。它的设计目标是识别与生物和化学风险相关的提示词，并指示模型拒绝提供这方面的建议。</p><p data-vmark="53c8">为了建立一个基准，OpenAI 的红队成员花费了大约 1000 小时，标记了 o3 和 o4-mini 中与生物风险相关的“不安全”对话。在模拟安全监控器的“阻断逻辑”测试中，<strong>模型拒绝回应风险提示的比例达到了 98.7%</strong>。然而，OpenAI 也承认，其测试并未考虑到用户在被监控器阻断后尝试新提示词的情况。因此，公司表示将继续依赖部分人工监控来弥补这一不足。</p><p data-vmark="f740">尽管 o3 和 o4-mini 并未达到 OpenAI 所设定的生物风险“高风险”阈值，但与 o1 和 GPT-4 相比，早期版本的 o3 和 o4-mini 在回答有关开发生物武器的问题上更具帮助性。根据 OpenAI 最近更新的准备框架，公司正在积极跟踪其模型可能如何帮助恶意用户更容易地开发化学和生物威胁。</p><p style="text-align: center;" data-vmark="1401"><img src="https://img.ithome.com/newsuploadfiles/2025/4/5ab3f0f6-41f3-4bec-9129-74093dd2e18b.jpg?x-bce-process=image/format,f_auto" w="1282" h="588" data-weibo="1" data-mpos="2,0"  data-vmark="3bec" class="no-alt-img"></p><p data-vmark="6e71">OpenAI 正在越来越多地依靠自动化系统来降低其模型带来的风险。例如，为了防止 GPT-4o 的原生图像生成器创建儿童性虐待材料（CSAM），OpenAI 表示其使用了与 o3 和 o4-mini 相似的推理监控器。</p><p data-vmark="8cd1">然而，一些研究人员对 OpenAI 的安全措施提出了质疑，认为该公司并没有像人们期望的那样重视安全问题。其中，OpenAI 的红队合作伙伴 Metr 表示，他们在测试 o3 的欺骗性行为基准时时间相对有限。此外，OpenAI 决定不为其本周早些时候发布的 GPT-4.1 模型发布安全报告。</p>