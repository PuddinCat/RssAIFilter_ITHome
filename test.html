<p data-vmark="958d">IT之家 4 月 12 日消息，金融时报（FT）昨日（4 月 11 日）发布博文，报道称 OpenAI 大幅压缩最新大型语言模型（LLM）安全测试时间，此前需要数月时间，而最新 o3 模型仅有<strong>几天时间。</strong></p><p data-vmark="09c4"><strong>竞争驱动，安全让步</strong></p><p data-vmark="5757">据八位知情人士透露，OpenAI 大幅压缩模型的安全测试时间，留给员工和第三方测试团队“评估”（evaluations，即测试模型风险和性能）时间仅有数天时间，<strong>而此前通常会耗时数月时间。</strong></p><p data-vmark="d784">IT之家援引博文介绍，OpenAI 面临来自 Meta、Google 及 xAI 等对手的激烈竞争，需快速推出新模型以维持市场优势。o3 模型计划最早下周发布，留给测试者安全检查时间不到一周，而此前 GPT-4 的测试期长达六个月。</p><p data-vmark="2116">一位测试过 GPT-4 的人士透露，过去安全测试更彻底，某些危险能力在测试两个月后才被发现，而如今竞争压力迫使公司追求速度，忽视潜在风险。</p><p data-vmark="9b6e"><strong>测试不足，监管缺位</strong></p><p data-vmark="d6e4">全球目前尚未统一 AI 安全测试标准，但欧盟《AI 法案》将于今年晚些时候上线，要求企业对其最强大的模型进行安全测试。</p><p data-vmark="b67d">AI Futures Project 负责人 Daniel Kokotajlo 表示，由于缺乏强制监管，企业不会主动披露模型的危险能力，竞争压力进一步加剧了风险。</p><p data-vmark="e90f">OpenAI 曾承诺构建定制模型版本，测试其潜在滥用风险，例如是否能协助制造更具传染性的生物病毒。</p><p data-vmark="e4ec">这种测试需投入大量资源，包括聘请外部专家、创建特定数据集并进行“微调”（fine-tuning）。但 OpenAI 仅对较老旧的模型进行有限微调，最新模型如 o1 和 o3-mini 未全面测试。前 OpenAI 安全研究员 Steven Adler 批评，若不兑现测试承诺，公众有权知情。</p><p data-vmark="8909" style="text-align: center;"><img src="https://img.ithome.com/newsuploadfiles/2025/4/86958553-4525-47e2-9f8b-1f17dc1fe592.jpg?x-bce-process=image/watermark,text_QUnnlJ_miJA,type_RlpMYW5UaW5nSGVp,size_20,color_ffffff77,skw_1,skc_00000011,g_7,blr_2,bls_2,x_8,y_8/format,f_auto" w="1024" h="768" data-weibo="0" data-vmark="5620" data-ai="1" data-mpos="2,2" ></p><p data-vmark="6caf"><strong>安全测试未覆盖最终模型</strong></p><p data-vmark="5185">另一问题在于，安全测试通常基于早期“检查点”（checkpoints），而非最终发布模型。一位前 OpenAI 技术人员表示，发布未经测试的更新模型是“不良做法”，而 OpenAI 辩称，其检查点与最终模型“基本一致”，并通过自动化测试提高效率，确保安全。</p>